SparkStreaming.scala
app.name=SparkStreaming
micro.batch.interval=10
bootstrap.servers=instance-1.us-east1-b.c.wired-effort-272101.internal:6667
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer
group.id=SparkStreaming
auto.offset.reset=earliest
security.protocol=PLAINTEXT
enable.auto.commit=true
auto.commit.interval.ms=5
client.id=SparkStreaming
outputDirectory=/tmp/Spark-Kafka-Stream/Output
topic.name=tp_in_profileupdates
eckpointDirectory=/tmp/Spark-Kafka-Stream/Checkpoint

#SparkStructStreaming
app.name=SparkStreaming
bootstrap.servers=instance-1.us-east1-b.c.wired-effort-272101.internal:6667
topic.name=tp_in_profileupdates


#DataProducer::For Kafka-Producer
topic.name=tp_in_profileupdates
bootstrap.servers=instance-1.us-east1-b.c.wired-effort-272101.internal:6667
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer
topic.key=user.id
message={"user_fname":"naga", "user_lname":"peeriga", "user_id":"test1.test", "updated_at":"2020-03-30 14:15:16.000", "modified_fields":[{"address":"new value"}]} 

#DataConsumer::For Kafka-Consumer Only
topic.name=tp_in_profileupdates
bootstrap.servers=instance-1.us-east1-b.c.wired-effort-272101.internal:6667
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
auto.offset.reset=earliest
group.id=GRP_TP_PROFILE_UPDATES


#FileSparkBatch
app.name=FileSparkBatch
file.stream.input.path=/tmp/Spark-Kafka-Stream/Output-AVRO/datepart=2020-03-22/*
outputDirectory=/tmp/Spark-Kafka-Stream/Output
input.date.format=yyyy-MM-dd kk:mm:ss

#FileSparkStream
app.name=FileSparkStream
micro.batch.interval=10
file.stream.input.path=/tmp/Spark-Kafka-Stream/Output-AVRO/datepart=2020-03-22/*
outputDirectory=/tmp/Spark-Kafka-Stream/Output

#FileSparkBatchHBase
app.name=FileSparkBatchHBase
file.stream.input.path=/tmp/Spark-Kafka-Stream/Output-AVRO/datepart=2020-03-22/*
outputDirectory=/tmp/Spark-Kafka-Stream/Output
input.date.format=yyyy-MM-dd kk:mm:ss

#KafkaStreaming(non-spark based)
source.topic=SourceTopic
sink.topic=SinkTopic
application.id=KStream-Training
default.key.serde=org.apache.kafka.common.serialization.StringSerializer
default.value.serde=org.apache.kafka.common.serialization.StringSerializer
source.topic=SourceTopic
sink.topic=SinkTopic

metadata.broker.list=instance-1.us-east1-b.c.wired-effort-272101.internal